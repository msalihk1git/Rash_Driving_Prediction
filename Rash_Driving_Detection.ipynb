{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,f2_score,roc_auc_score,roc_auc_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "df=pd.read_csv(\"rash_driving_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basics EDA(Exploratory data analysis)\n",
    "df.head()         #get the first five rows\n",
    "df.tail()         #get the last five rows\n",
    "df.info()         #dataset information\n",
    "df.shape()        #shape of dataset\n",
    "df.describe()     #describe the dataset\n",
    "df.isna().sum()   #checking the missing values\n",
    "df['column'].fillna(value=mean_value, inplace=True)         #replace null values with mean     -missing value treatment\n",
    "df.corr()[\"Outcome\"]                      #Correlation on Dataset based on target variable\n",
    "sns.pairplot(df,hue=\"Outcome\")            #Pairplot using sns -plot relationship between variables in a dataset    ,here outcome column nmae is \"rash_driving\"\n",
    "sns.boxplot(df[\"column_name\"])            #checking for outliers\n",
    "df['column_name'].mean()+df['column_name'].std()*3      #outlier treatment using mean,standard deviation,percentiles\n",
    "\n",
    "#train_test_split\n",
    "X=df.drop([\"outcome\"],axis=1)   # X contains independent variables\n",
    "y=df[\"outcome\"]     #y is the output variable, here the output column is \"rash_driving\"\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)        #train_test_split \n",
    "\n",
    "#scale down the dataset using standard scalar\n",
    "scale=StandardScaler()\n",
    "X_train_scaled=scale.fit_transform(X_train)\n",
    "X_test_scaled=scale.transform(X_test)\n",
    "\n",
    "#fit the model \n",
    "rf=RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "rf.fit(X_train,y_train)\n",
    "#Predict on test set\n",
    "y_pred=rf.predict(X_test)\n",
    "\n",
    "#Evaluate the model\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculate  and print metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n",
    "print(\"AUC-ROC score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying different algorithms\n",
    "log =LogisticRegression()\n",
    "dtc =DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "adb = AdaBoostClassifier()\n",
    "grad = GradientBoostingClassifier()\n",
    "svc = SVC()\n",
    "knn = KNeighborsClassifier()\n",
    "nbc = GaussianNB()\n",
    "\n",
    "voting=VotingClassifier(estimators=[(\"Logistic_Regression\",log),(\"Decision_Tree_Classifier\",dtc),(\"Random_Forest_Classifier\",rf),(\"AdaBoost_Classifier\",adb),(\"Gragient_Boosting_Classifier\",grad),(\"SVC\",svc),(\"KNeighborsClassifier\",knn),(\"GaussianNB\",nbc)],voting=\"hard\")\n",
    "\n",
    "voting.fit(X_train,y_train)\n",
    "\n",
    "#fit with different models\n",
    "for  clf in (log,dtc,rf,adb,grad,svc,knn,nbc,voting):\n",
    "  clf.fit(X_train,y_train)\n",
    "  y_pred=clf.predict(X_test)\n",
    "  print(clf.__class__.__name__,clf.score(X_train,y_train))\n",
    "  print(clf.__class__.__name__,accuracy_score(y_test,y_pred))\n",
    "  \n",
    "  print(\"===========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "lr_clf = LogisticRegression()\n",
    "df_clf= DecisionTreeClassifier()\n",
    "rf_clf = RandomForestClassifier()\n",
    "adboost_clf = AdaBoostClassifier()\n",
    "grad_clf = GradientBoostingClassifier()\n",
    "svc_clf = SVC()\n",
    "xgb_clf = XGBClassifier()\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "clf_list=[lr_clf,df_clf,rf_clf,adboost_clf,grad_clf,svc_clf,xgb_clf,knn_clf]\n",
    "\n",
    "grid_params_lr= [{'penalty':['l1','l2'],'solver':['saga']}]\n",
    "\n",
    "grid_params_df =[{'criterion':[\"gini\",\"entropy\"], 'splitter':['best','random'],'max_depth':[3,4,5],'min_samples_split':[2,3,4],'max_features':[\"auto\",\"sqrt\",\"log2\"]}]\n",
    "\n",
    "grid_params_rf=[{'n_estimators': [4, 6, 9], 'max_features': ['log2', 'sqrt','auto'], 'criterion': ['entropy', 'gini'],'max_depth': [2, 3, 5, 10]}]\n",
    "\n",
    "grid_params_adboost=[{'n_estimators':[10,50,250,1000],'learning_rate':[0.01,0.1],}]\n",
    "\n",
    "grid_params_grad=[{'loss':['deviance', 'exponential'],'learning_rate':[1,7,9],'criterion':['friedman_mse','mse']}]\n",
    "\n",
    "grid_params_svc=[{'kernel':['linear','poly','rbf'],'degree':[3,4,5]}]\n",
    "\n",
    "grid_params_xgb=[{'booster':['gbtree', 'gblinear'],'nthread':[10,15,25]}]\n",
    "\n",
    "grid_params_knn=[{'n_neighbors':[5,7,9,11],'algorithm':['ball_tree','kd_tree','brute'],'leaf_size':[30,50,100]}]\n",
    "\n",
    "\n",
    "clf_params=[grid_params_lr,grid_params_df,grid_params_rf,grid_params_adboost,grid_params_grad,grid_params_svc,grid_params_xgb,grid_params_knn]\n",
    "\n",
    "\n",
    "\n",
    "for clf,clf_param in zip(clf_list,clf_params):\n",
    "  print(f\"The Classifier is {clf} and its hyper params are {clf_param}\")\n",
    "\n",
    "  grid_clf = GridSearchCV(estimator=clf,param_grid=clf_param,scoring=\"accuracy\",cv=10)\n",
    "  grid_clf.fit(X_train_scaled,y_train)\n",
    "  print(f\"The Train accuracy for the {clf} is {grid_clf.score(X_train_scaled,y_train)}\")\n",
    "  print(f\"The Test accuracy for the {clf} is {grid_clf.score(X_test_scaled,y_test)}\")\n",
    "  print(f\"The best param for {clf} is {grid_clf.best_params_}\")\n",
    "  print(\"====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the best model on the basis of accuracy, here we assume random forest is best model\n",
    "\n",
    "rf_clf = RandomForestClassifier(criterion = 'entropy',max_depth = 3,max_features = 'log2',n_estimators = 4)\n",
    "rf_clf.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making pickle file of the model \n",
    "#Pickle file\n",
    "import joblib   \n",
    "joblib.dump(rf_clf,'filepath.pkl')     #pickle  file of the model\n",
    "joblib.dump(scale,'filepath.pk1')      #Pickle file of the scaled data \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916e4c36f1c77e641c78c5aec4d67a7d0f7bb4a72f03f76476ea0be2966b92cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
